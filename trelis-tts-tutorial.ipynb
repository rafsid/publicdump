{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Curation for StyletTTS2\n",
    "\n",
    "This Jupyter notebook guide shows teh process of creating an audio dataset from YouTube videos or from your own audio dataset. It covers downloading audio, converting formats, transcribing, segmenting, and preparing the data for upload to Huggingface\n",
    "\n",
    "\n",
    "Attribution: This notebook builds on scrpts form the following repos:\n",
    "- WhisperX\n",
    "- StyleTTS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install uv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and Dependencies\n",
    "\n",
    "First, we need to install all the required dependencies. Run the following cell to install the necessary packages and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://tender-months-sip.loca.lt/k/208093503/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..Felfp8NTthaQ3YDIDhT53w.5uX2IS1Bxc1HK4oftj9YrCl6khs-ywYUlevW78hjtXCyUqxtHsSW9LkIFKnMBdHcs_TToDB6N50-tySNxPZYyNIo7nEUmS65u5IE_jRpmVA6Ltt5yIcbU18-GAoFvGFQ4TxHMPRlbfyFlR9Llalapq5Vg820iCFtcqPpwtVYmlzytOVXQaBjmxmh_352mwinefPSQ88ZXxOBbTpwJDO9dg.EQhkzSUGbIjXTWWsy-pgEA/proxy'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# # !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -qU\n",
    "\n",
    "# !apt-get update -qq > /dev/null # update quietly\n",
    "# !apt-get install espeak-ng -y > /dev/null # install quietly\n",
    "# !uv pip install git+https://github.com/m-bain/whisperX.git -q --system # you may be required to re-start the runtime during installation\n",
    "# !uv pip install phonemizer pysqrt -q --system\n",
    "# !uv pip install pydub -qU --system\n",
    "# !uv pip install pytube -qU --system\n",
    "# !uv pip install tqdm -q --system\n",
    "# # !git clone https://github.com/rs54837/StyleTTS2FineTune.git\n",
    "# !apt-get install ffmpeg -y > /dev/null #install quietyly\n",
    "# !uv pip install whisperx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -qU\n",
    "\n",
    "!apt-get update -qq > /dev/null # update quietly\n",
    "!apt-get install espeak-ng -y > /dev/null # install quietly\n",
    "!pip install git+https://github.com/m-bain/whisperX.git -q # you may be required to re-start the runtime during installation\n",
    "!pip install phonemizer pysqrt -q\n",
    "!pip install pydub -qU\n",
    "!pip install pytube -qU\n",
    "!pip install tqdm -q\n",
    "# !git clone https://github.com/rs54837/StyleTTS2FineTune.git\n",
    "!apt-get install ffmpeg -y > /dev/null #install quietyly\n",
    "!pip install whisperx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For macos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Homebrew if not already installed\n",
    "# /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "\n",
    "# # Update Homebrew\n",
    "# brew update\n",
    "\n",
    "# # Install espeak-ng and ffmpeg\n",
    "# brew install espeak-ng ffmpeg\n",
    "\n",
    "# # (Optional) Install PyTorch with CUDA support\n",
    "# # Uncomment the following line if you need GPU support and have CUDA installed\n",
    "# # pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -qU\n",
    "\n",
    "# # Create a Python virtual environment named 'tts_env'\n",
    "# python3 -m venv tts_env\n",
    "\n",
    "# # Activate the virtual environment\n",
    "# source tts_env/bin/activate\n",
    "\n",
    "# # Upgrade pip within the virtual environment\n",
    "# pip install --upgrade pip\n",
    "\n",
    "# # Install Python packages\n",
    "# pip install git+https://github.com/m-bain/whisperX.git -q\n",
    "# pip install phonemizer pysqrt -q\n",
    "# pip install pydub --upgrade -q\n",
    "# pip install pytube --upgrade -q\n",
    "# pip install tqdm -q\n",
    "\n",
    "# # (Optional) Clone the StyleTTS2FineTune repository\n",
    "# # Uncomment the following line if you need to clone the repository\n",
    "# # git clone https://github.com/rs54837/StyleTTS2FineTune.git\n",
    "\n",
    "# # Install whisperx (if not already installed via the git repository)\n",
    "# pip install whisperx -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Creating all the required directories\n",
    "\n",
    "srtsegmenter.py file creates the directories needed to process all the audio wav files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to restart\n",
    "!rm -r paddedAudio segmentedAudio offLengthAudio trainingdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "output_dir = './segmentedAudio/' # path to save segmented audio files\n",
    "off_length_audio_dir = './offLengthAudio/' # path to save off-length audio files\n",
    "srt_dir = './srt/' # path to save srt files\n",
    "audio_dir = './audio/' # path to save audio files\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(off_length_audio_dir, exist_ok=True) # only required if doing simple segmentation\n",
    "os.makedirs(srt_dir, exist_ok=True)\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "os.makedirs('./trainingdata', exist_ok=True)\n",
    "\n",
    "srt_list = glob.glob(\".srt/*.srt\") # Gets all srt files in the current directory\n",
    "audio_list = glob.glob(\"./audio/*.wav\") # Gets all audio files in the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Audio from YouTube\n",
    "\n",
    "This section allows you to download audio from  YouTube videos and conver the audio to wav format. Replace the youtube_link variable with the URL of the video you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download an audio wav file from youtube video link and store it in audio folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yt_dlp -qU #you can also use pytube but there are some issues with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=lJDxkjE9SSY\n",
      "[youtube] lJDxkjE9SSY: Downloading webpage\n",
      "[youtube] lJDxkjE9SSY: Downloading ios player API JSON\n",
      "[youtube] lJDxkjE9SSY: Downloading mweb player API JSON\n",
      "[youtube] lJDxkjE9SSY: Downloading m3u8 information\n",
      "[info] lJDxkjE9SSY: Downloading 1 format(s): 251\n",
      "[download] Destination: audio/My TOP TEN TIPS for Fine-tuning.webm\n",
      "[download] 100% of   23.54MiB in 00:00:00 at 46.52MiB/s    \n",
      "[ExtractAudio] Destination: audio/My TOP TEN TIPS for Fine-tuning.mp3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mInvalid response: 500 Internal Server Error. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "def download_audio(youtube_link, output_path=\"audio\"):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': f'{output_path}/%(title)s.%(ext)s',\n",
    "    }\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([youtube_link])\n",
    "    print(f\"Audio downloaded from {youtube_link} and saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "# youtube_link = \"https://www.youtube.com/watch?v=c-muF16kLSQ&ab\"\n",
    "youtube_link = \"https://www.youtube.com/watch?v=lJDxkjE9SSY\"\n",
    "download_audio(youtube_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from mp3 Audio to WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to convert: audio/My TOP TEN TIPS for Fine-tuning.mp3\n",
      "Converted My TOP TEN TIPS for Fine-tuning.mp3 to audio/topten.wav\n",
      "Deleted My TOP TEN TIPS for Fine-tuning.mp3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "    \n",
    "def convert_to_wav(input_file, output_file):\n",
    "    audio = AudioSegment.from_file(\n",
    "        input_file,\n",
    "        format=\"mp3\"\n",
    "    )\n",
    "    audio.export(output_file, format=\"wav\")\n",
    "\n",
    "# Get the mp3 files\n",
    "audio_folder = \"audio\"\n",
    "# input_files = [f for f in os.listdir(audio_folder) if f.endswith(\".mp4\")]\n",
    "input_files = [f for f in os.listdir(audio_folder) if f.endswith(\".mp3\")]\n",
    "\n",
    "if input_files:\n",
    "    for input_file in input_files:\n",
    "        input_file_path = os.path.join(audio_folder, input_file)\n",
    "        print(f\"Attempting to convert: {input_file_path}\")  # Debug print\n",
    "        \n",
    "        try:\n",
    "            # Ask for a new filename\n",
    "            new_name = input(f\"Enter a new filename for {input_file} (without extension): \")\n",
    "            output_file_path = os.path.join(audio_folder, f\"{new_name}.wav\")\n",
    "            \n",
    "            # Convert to wav\n",
    "            convert_to_wav(input_file_path, output_file_path)\n",
    "            print(f\"Converted {input_file} to {output_file_path}\")\n",
    "            \n",
    "            # Delete the original file\n",
    "            # os.remove(input_file_path)\n",
    "            print(f\"Deleted {input_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\t audio\t\t pankaj.json  pankaj.vtt      state.db\n",
      "Untitled1.ipynb  install.sh\t pankaj.tsv   segmentedAudio  trainingdata\n",
      "Untitled2.ipynb  offLengthAudio  pankaj.txt   srt\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/utils/_process_posix.py:151\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    152\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPtyProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd audio/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"audio\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1472, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/opt/conda/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n",
      "    from ..image_processing_utils import BaseImageProcessor\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py\", line 28, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/image_transforms.py\", line 47, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/__init__.py\", line 45, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import distribute\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.distribute.combinations import env # line: 456\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/combinations.py\", line 33, in <module>\n",
      "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 25, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_ops as cross_device_ops_lib\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 28, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_utils\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/cross_device_utils.py\", line 22, in <module>\n",
      "    from tensorflow.python.distribute import values as value_lib\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/values.py\", line 23, in <module>\n",
      "    from tensorflow.python.distribute import distribute_lib\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 205, in <module>\n",
      "    from tensorflow.python.data.ops import dataset_ops\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.data import experimental\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/__init__.py\", line 97, in <module>\n",
      "    from tensorflow.python.data.experimental import service\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
      "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 26, in <module>\n",
      "    from tensorflow.python.data.ops import dataset_ops\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 34, in <module>\n",
      "    from tensorflow.python.data.ops import iterator_ops\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 41, in <module>\n",
      "    from tensorflow.python.training.saver import BaseSaverBuilder\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/training/saver.py\", line 50, in <module>\n",
      "    from tensorflow.python.training import py_checkpoint_reader\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 19, in <module>\n",
      "    from tensorflow.python.util._pywrap_checkpoint_reader import CheckpointReader\n",
      "SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/whisperx\", line 5, in <module>\n",
      "    from whisperx.transcribe import cli\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisperx/__init__.py\", line 1, in <module>\n",
      "    from .transcribe import load_model\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisperx/transcribe.py\", line 10, in <module>\n",
      "    from .asr import load_model\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisperx/asr.py\", line 9, in <module>\n",
      "    from transformers import Pipeline\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1462, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1474, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
      "initialization of _pywrap_checkpoint_reader raised unreported exception\n",
      "RuntimeError: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1472, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/opt/conda/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n",
      "    from ..image_processing_utils import BaseImageProcessor\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py\", line 28, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/image_transforms.py\", line 47, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/__init__.py\", line 45, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import distribute\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.distribute.combinations import env # line: 456\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/combinations.py\", line 33, in <module>\n",
      "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 25, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_ops as cross_device_ops_lib\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 28, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_utils\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/cross_device_utils.py\", line 22, in <module>\n",
      "    from tensorflow.python.distribute import values as value_lib\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/values.py\", line 23, in <module>\n",
      "    from tensorflow.python.distribute import distribute_lib\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 205, in <module>\n",
      "    from tensorflow.python.data.ops import dataset_ops\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.data import experimental\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/__init__.py\", line 97, in <module>\n",
      "    from tensorflow.python.data.experimental import service\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
      "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 26, in <module>\n",
      "    from tensorflow.python.data.ops import dataset_ops\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 34, in <module>\n",
      "    from tensorflow.python.data.ops import iterator_ops\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 41, in <module>\n",
      "    from tensorflow.python.training.saver import BaseSaverBuilder\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/training/saver.py\", line 50, in <module>\n",
      "    from tensorflow.python.training import py_checkpoint_reader\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 19, in <module>\n",
      "    from tensorflow.python.util._pywrap_checkpoint_reader import CheckpointReader\n",
      "SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/whisperx\", line 5, in <module>\n",
      "    from whisperx.transcribe import cli\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisperx/__init__.py\", line 1, in <module>\n",
      "    from .transcribe import load_model\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisperx/transcribe.py\", line 10, in <module>\n",
      "    from .asr import load_model\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisperx/asr.py\", line 9, in <module>\n",
      "    from transformers import Pipeline\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1462, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1474, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
      "initialization of _pywrap_checkpoint_reader raised unreported exception\n",
      "mv: cannot stat '*.srt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!for i in  ./audio/*.wav; do whisperx \"$i\" --model large-v2 --align_model WAV2VEC2_ASR_LARGE_LV60K_960H; done\n",
    "!mv *.srt ./srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "whisperx 3.1.1 requires pyannote.audio==3.1.1, but you have pyannote-audio 3.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement openai-whisper-qU (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for openai-whisper-qU\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 24.8.3 requires cubinlinker, which is not installed.\n",
      "cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 24.8.3 requires ptxcompiler, which is not installed.\n",
      "cuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "ucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\n",
      "albucore 0.0.16 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
      "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "arviz 0.19.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
      "bayesian-optimization 1.5.1 requires numpy>=1.25, but you have numpy 1.22.0 which is incompatible.\n",
      "beatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.5 which is incompatible.\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "cesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.22.0 which is incompatible.\n",
      "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\n",
      "cudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\n",
      "cudf 24.8.3 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "cudf 24.8.3 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "dask-cuda 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "dask-cudf 24.8.3 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "dask-cudf 24.8.3 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "dask-expr 1.1.14 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\n",
      "dipy 1.9.0 requires numpy>=1.22.4, but you have numpy 1.22.0 which is incompatible.\n",
      "distributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\n",
      "featuretools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.22.0 which is incompatible.\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\n",
      "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\n",
      "jupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "mizani 0.11.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
      "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
      "mne 1.8.0 requires numpy<3,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
      "plotnine 0.13.6 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
      "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
      "pyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.22.0 which is incompatible.\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "pylibraft 24.8.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "pywavelets 1.6.0 requires numpy<3,>=1.22.4, but you have numpy 1.22.0 which is incompatible.\n",
      "raft-dask 24.8.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "rapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\n",
      "rmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\n",
      "rmm 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "scikit-image 0.23.2 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "statsmodels 0.14.2 requires numpy>=1.22.3, but you have numpy 1.22.0 which is incompatible.\n",
      "tensorflow 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.22.0 which is incompatible.\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
      "ucx-py 0.39.2 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "ucxx 0.39.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "whisperx 3.1.1 requires pyannote.audio==3.1.1, but you have pyannote-audio 3.3.2 which is incompatible.\n",
      "woodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.22.0 which is incompatible.\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "xarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
      "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
      "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pyannote.audio -qU\n",
    "# %pip install pydub -qU\n",
    "%pip install openai-whisper -qU\n",
    "%pip install TTS -qU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "# Replace with your Hugging Face access token\n",
    "HUGGINGFACE_TOKEN = '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'\n",
    "\n",
    "# Initialize the speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization\",\n",
    "    use_auth_token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "# Apply the pipeline to the podcast audio file\n",
    "diarization = pipeline(\"podcast_audio.wav\")\n",
    "\n",
    "# Print out the diarization results\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"Start: {turn.start:.1f}s, End: {turn.end:.1f}s, Speaker: {speaker}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the original podcast audio\n",
    "audio = AudioSegment.from_wav(\"podcast_audio.wav\")\n",
    "\n",
    "# Specify the target speaker label (e.g., 'SPEAKER_00')\n",
    "target_speaker = 'SPEAKER_00'\n",
    "\n",
    "# Directory to save the target speaker's audio segments\n",
    "os.makedirs('target_speaker_segments', exist_ok=True)\n",
    "\n",
    "segment_counter = 0\n",
    "\n",
    "# Iterate over diarization results and extract target speaker segments\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    if speaker == target_speaker:\n",
    "        # Convert time to milliseconds for pydub\n",
    "        start_ms = int(turn.start * 1000)\n",
    "        end_ms = int(turn.end * 1000)\n",
    "        segment_audio = audio[start_ms:end_ms]\n",
    "        segment_path = f'target_speaker_segments/segment_{segment_counter}.wav'\n",
    "        segment_audio.export(segment_path, format=\"wav\")\n",
    "        segment_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import glob\n",
    "\n",
    "# Load the Whisper model (you can choose 'tiny', 'base', 'small', 'medium', 'large')\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# List to store transcripts\n",
    "transcripts = []\n",
    "\n",
    "# Get all extracted audio segment files\n",
    "segment_files = sorted(glob.glob('target_speaker_segments/*.wav'))\n",
    "\n",
    "for segment_file in segment_files:\n",
    "    # Transcribe the audio segment\n",
    "    result = model.transcribe(segment_file)\n",
    "    text = result['text'].strip()\n",
    "    transcripts.append({\n",
    "        'audio_file': segment_file,\n",
    "        'transcript': text\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metadata file required for TTS training\n",
    "with open('metadata.csv', 'w', encoding='utf-8') as f:\n",
    "    for item in transcripts:\n",
    "        # The format is: audio_file_path|transcript\n",
    "        f.write(f\"{item['audio_file']}|{item['transcript']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from TTS.utils.manage import ModelManager\n",
    "from TTS.trainer import Trainer\n",
    "\n",
    "# Download a pre-trained TTS model\n",
    "model_manager = ModelManager()\n",
    "model_path, config_path, model_item = model_manager.download_model('tts_models/en/ljspeech/tacotron2-DDC')\n",
    "\n",
    "# Load the model configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update the configuration for fine-tuning\n",
    "config['output_path'] = 'tts_output'\n",
    "config['datasets'] = [{\n",
    "    'name': 'fine_tuning_dataset',\n",
    "    'path': '.',  # Current directory\n",
    "    'meta_file_train': 'metadata.csv',\n",
    "    'meta_file_val': 'metadata.csv',  # You can split your data into train/validation sets\n",
    "}]\n",
    "\n",
    "# Save the updated configuration\n",
    "with open('config_modified.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    config_path='config_modified.yaml',\n",
    "    model_path=model_path,\n",
    "    output_path=config['output_path']\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
