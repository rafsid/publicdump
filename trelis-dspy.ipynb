{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# try: # When on google Colab, let's clone the notebook so we download the cache.\n",
    "#     import google.colab  # noqa: F401\n",
    "#     repo_path = 'dspy'\n",
    "#     !git -C $repo_path pull origin || git clone https://github.com/stanfordnlp/dspy $repo_path\n",
    "# except:\n",
    "#     repo_path = '.'\n",
    "repo_path = 'dspy'\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Set up the cache for this notebook\n",
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(repo_path, 'cache')\n",
    "\n",
    "import pkg_resources # Install the package if it's not installed\n",
    "if \"dspy-ai\" not in {pkg.key for pkg in pkg_resources.working_set}:\n",
    "    !pip install -U pip\n",
    "    !pip install dspy-ai==2.4.17\n",
    "    !pip install openai~=0.28.1\n",
    "    # !pip install -e $repo_path\n",
    "\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import dspy\n",
    "\n",
    "api_key = \"dummy\"\n",
    "# mini = dspy.OpenAI(model='gpt-4o-mini') \n",
    "# groq.api_key = \"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\"\n",
    "\n",
    "\n",
    "mini = dspy.LM(\n",
    "    model=\"openai/qwen2.5-3b-instruct-mlx\",  # Add 'openai/' prefix\n",
    "    api_base=\"http://localhost:1234/v1\",\n",
    "    api_key=\"api_key\",\n",
    "    max_tokens=5000,\n",
    "    use_completion_for_chat=True\n",
    ")\n",
    "dspy.settings.configure(lm=mini)\n",
    "\n",
    "p = dspy.Predict(\"question -> answer\")\n",
    "p(question=\"What is the capital of France?\")\n",
    "\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "dspy.settings.configure(lm=mini, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini = dspy.LM(\n",
    "#     model=\"openai/qwen2.5-3b-instruct-mlx\",  # Add 'openai/' prefix\n",
    "#     api_base=\"http://localhost:1234/v1\",\n",
    "#     api_key=\"sk-fake-key\",\n",
    "#     max_tokens=5000,\n",
    "#     use_completion_for_chat=True\n",
    "# )\n",
    "# dspy.settings.configure(lm=mini)\n",
    "\n",
    "# p = dspy.Predict(\"question -> answer\")\n",
    "# p(question=\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "# Load the dataset.\n",
    "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n",
    "\n",
    "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = trainset[0]\n",
    "print(f\"Question : {train_example.question}\")\n",
    "print(f\"Answer: {train_example.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_example = devset[18]\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "print(f\"Answer: {dev_example.answer}\")\n",
    "print(f\"Relevant Wikipedia Titles: {dev_example.gold_titles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For this dataset, training examples have input keys {train_example.inputs().keys()} and label keys {train_example.labels().keys()}\")\n",
    "print(f\"For this dataset, dev examples have input keys {dev_example.inputs().keys()} and label keys {dev_example.labels().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "    \n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predictor.\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "# Call the predictor on a particular input.\n",
    "pred = generate_answer(question=dev_example.question)\n",
    "\n",
    "# Print the input and the prediction.\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "\n",
    "print(f\"pred: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Modify the LM configuration to use text completion mode\n",
    "mini = dspy.LM(\n",
    "    model=\"openai/qwen2.5-3b-instruct-mlx\",\n",
    "    api_base=\"http://localhost:1234/v1\",\n",
    "    api_key=\"sk-fake-key\",\n",
    "    max_tokens=5000,\n",
    "    use_completion_for_chat=True,\n",
    "    chat_mode=False  # Add this to force text completion mode\n",
    ")\n",
    "\n",
    "# Option 2: Create a custom adapter that handles raw text responses\n",
    "class SimpleTextAdapter(dspy.Adapter):\n",
    "    def format(self, signature, demos, inputs):\n",
    "        # Format the prompt as needed\n",
    "        question = inputs['question']\n",
    "        return f\"Question: {question}\\nAnswer:\"\n",
    "    \n",
    "    def parse(self, signature, completion):\n",
    "        # Parse raw text into the expected format\n",
    "        return {\"answer\": completion.strip()}\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=mini,\n",
    "    adapter=SimpleTextAdapter()  # Use the custom adapter\n",
    ")\n",
    "\n",
    "# Then use your existing code\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "pred = generate_answer(question=dev_example.question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate_on_hotpotqa(generate_answer, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the predictor. Notice we're just changing the class. The signature BasicQA is unchanged.\n",
    "# generate_answer_with_chain_of_thought = dspy.ChainOfThought(BasicQA)\n",
    "\n",
    "# # Call the predictor on the same input.\n",
    "# pred = generate_answer_with_chain_of_thought(question=dev_example.question)\n",
    "\n",
    "# # Print the input, the chain of thought, and the prediction.\n",
    "# print(f\"Question: {dev_example.question}\")\n",
    "# print(f\"Thought: {pred.rationale.split('.', 1)[1].strip()}\")\n",
    "# print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextAdapter(dspy.Adapter):\n",
    "    def format(self, signature, demos, inputs):\n",
    "        if 'reasoning' in signature.output_fields:\n",
    "            # Format for chain-of-thought\n",
    "            return f\"Question: {inputs['question']}\\nLet's think about this step by step:\\n1.\"\n",
    "        else:\n",
    "            # Format for direct answer\n",
    "            return f\"Question: {inputs['question']}\\nAnswer:\"\n",
    "    \n",
    "    def parse(self, signature, completion):\n",
    "        if 'reasoning' in signature.output_fields:\n",
    "            # Split into reasoning and answer\n",
    "            parts = completion.split('\\nTherefore, ')\n",
    "            if len(parts) == 2:\n",
    "                reasoning, answer = parts\n",
    "            else:\n",
    "                # Handle case where model doesn't use \"Therefore\"\n",
    "                lines = completion.strip().split('\\n')\n",
    "                reasoning = '\\n'.join(lines[:-1])\n",
    "                answer = lines[-1].replace('Answer:', '').strip()\n",
    "            \n",
    "            return {\n",
    "                \"reasoning\": reasoning.strip(),\n",
    "                \"answer\": answer.strip()\n",
    "            }\n",
    "        else:\n",
    "            # Just return answer for basic queries\n",
    "            return {\"answer\": completion.strip()}\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=mini,\n",
    "    adapter=SimpleTextAdapter()\n",
    ")\n",
    "\n",
    "# Now try the chain of thought version again\n",
    "generate_answer_with_chain_of_thought = dspy.ChainOfThought(BasicQA)\n",
    "pred = generate_answer_with_chain_of_thought(question=dev_example.question)\n",
    "\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "if hasattr(pred, 'reasoning'):\n",
    "    print(f\"Thought: {pred.reasoning}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate_on_hotpotqa(generate_answer_with_chain_of_thought, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve = dspy.Retrieve(k=3)\n",
    "topK_passages = retrieve(dev_example.question).passages\n",
    "\n",
    "print(f\"Top {retrieve.k} passages for question: {dev_example.question} \\n\", '-' * 30, '\\n')\n",
    "\n",
    "for idx, passage in enumerate(topK_passages):\n",
    "    print(f'{idx+1}]', passage, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve(\"When was the first FIFA World Cup held?\").passages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_model = RAG()\n",
    "pred = rag_model('When was the first rugby world cup?')\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate_on_hotpotqa(rag_model, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import LabeledFewShot\n",
    "\n",
    "labeled_fewshot_optimizer = LabeledFewShot(k=8)\n",
    "rag_model_compiled = labeled_fewshot_optimizer.compile(student = rag_model, trainset = trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate_on_hotpotqa(rag_model_compiled, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
    "\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(RAG(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
    "    \n",
    "def rate_limit_decorator(max_per_minute=28):  # Set slightly below limit of 30\n",
    "    min_interval = 60.0 / max_per_minute\n",
    "    last_called = [0.0]  # Using list to modify in closure\n",
    "\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            now = time.time()\n",
    "            elapsed = now - last_called[0]\n",
    "            if elapsed < min_interval:\n",
    "                sleep_time = min_interval - elapsed\n",
    "                time.sleep(sleep_time)\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                last_called[0] = time.time()\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e) or \"Connection error\" in str(e):\n",
    "                    sleep_time = random.uniform(1, 3)  # Random backoff\n",
    "                    time.sleep(sleep_time)\n",
    "                    return func(*args, **kwargs)  # Retry once\n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Apply rate limiting to LM\n",
    "class RateLimitedLM(dspy.LM):\n",
    "    @rate_limit_decorator()\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super().__call__(*args, **kwargs)\n",
    "\n",
    "# Configure DSPy with rate-limited LM\n",
    "mini = RateLimitedLM(\n",
    "    model=\"openai/qwen2.5-3b-instruct-mlx\",\n",
    "    api_base=\"http://localhost:1234/v1\",\n",
    "    api_key=\"sk-fake-key\",\n",
    "    max_tokens=5000,\n",
    "    use_completion_for_chat=True,\n",
    "    num_retries=3  # Add retries for robustness\n",
    ")\n",
    "\n",
    "# Configure DSPy settings with lower batch sizes\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "dspy.settings.configure(\n",
    "    lm=mini,\n",
    "    rm=colbertv2_wiki17_abstracts,\n",
    "    max_batch_size=5,  # Reduce batch size to avoid hitting rate limits\n",
    ")\n",
    "\n",
    "\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Then use your teleprompter with smaller batch sizes\n",
    "teleprompter = BootstrapFewShot(\n",
    "    metric=validate_context_and_answer,\n",
    "    max_bootstrapped_demos=5,  # Reduce from default\n",
    "    max_labeled_demos=5,  # Reduce from default\n",
    "    # max_batch_size=3  # Control concurrent requests\n",
    ")\n",
    "\n",
    "# Compile with rate limiting in place\n",
    "compiled_rag = teleprompter.compile(RAG(), trainset=trainset[:10])  # Start with smaller trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
